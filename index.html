<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Qianli Ma / 马千里</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism-solarizedlight.min.css"/> -->
  <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/prism.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.min.js"></script> -->
  <link rel="icon" type="image/png" href="images/icon.png">

  <!-- <script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js"></script> -->

</head>

<body>
  <table style="width:100%;max-width:1040px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:left">
                <name>Qianli Ma</name>
              </p>
              <p>I am a PhD student at the <a href="https://ps.is.tuebingen.mpg.de/">Max Planck Institute for Intelligent Systems</a>
                and <a href="https://vlg.inf.ethz.ch/">ETH Zürich</a>, co-supervised by 
                <a href="https://ps.is.tuebingen.mpg.de/person/black">Michael Black</a> and 
                <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang </a>.
                I am also associated to the <a href="https://learning-systems.org/">Max Planck ETH Center for Learning Systems</a>.
                Prior to this, I received my Master's degree in Optics and Photonics from Karlsruhe Institute of Technology
                and Bachelor's degree in Physics from Peking University.
              </p>
              <p>
              My research uses machine learning to solve computer vision and graphics problems, with 
              a current focus on 3D representations and deformable 3D shape modeling.
              </p>
              <p style="text-align:left">
                <a href="mailto:qma@tue.mpg.de"><i class="fa fa-paper-plane"></i>&nbsp&nbspEmail</a></a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ks-bHqsAAAAJ&hl=en"><i class="ai ai-google-scholar ai-fw" style="font-size: 1.3em;position: relative; top:0.1em;margin-left: -0.3em;"></i>Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://scholar.google.com/citations?user=ks-bHqsAAAAJ&hl=en"><i class="ai ai-google-scholar ai-fw" style="font-size: 1.3em;position: relative; top:0.1em;margin-left: -0.3em;"></i>Google Scholar</a> &nbsp/&nbsp -->
                <a href="https://twitter.com/qianli_m"><i class="fa fa-twitter"></i>&nbsp&nbspTwitter</a> &nbsp/&nbsp
                <a href="https://github.com/qianlim/"><i class="fa fa-github"></i>&nbsp&nbspGithub</a>
                
              </p>
            </td>
            <td align="middle", style="padding:0%;width:36%;max-width:36%;vertical-align:right">
              <img style="width:82%;max-width:82%" alt="profile photo" src="images/me.jpg">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/SkiRT.png' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://qianlim.github.io/SkiRT">
                <font color=#1772d0>  <papertitle>Neural Point-based Shape Modeling of Humans in Challenging Clothing</papertitle></font>
              </a>
              <br>
              <strong>Qianli Ma</strong>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>, 
              <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
              <br>
              <em>3DV</em>, 2022
              <br>
              <a href="https://qianlim.github.io/SkiRT">Project Page</a> /
              <a href="https://github.com/qianlim/SkiRT">Code</a> /
              <a href="https://arxiv.org/abs/2209.06814">arXiv</a> /
              <!-- <a href="https://youtu.be/JY5OI74yJ4w">Video</a> -->
              <button id="bib_button" class="link", onclick="showBib('SkiRT_bib')">BibTex</button>
              <div id='SkiRT_bib' hidden>
              <pre><code>@inproceedings{SkiRT:3DV:2022,
  title = {Neural Point-based Shape Modeling of Humans in Challenging Clothing},
  author = {Ma, Qianli and Yang, Jinlong and Black, Michael J. and Tang, Siyu},
  booktitle = {International Conference on 3D Vision (3DV)},
  month = sep,
  year = {2022},
}</code></pre>
              </div>

              <p></p>
              <p>The power of point-based digital human representations further unleashed: <i>SkiRT</i> models dynamic shapes of 3D clothed
                humans including those that wear challenging outfits such as skirts and dresses. 
                </p> 
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/EgoBody4.png' id='egobody_image' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sanweiliti.github.io/egobody/egobody.html">
                <papertitle>EgoBody: Human Body Shape and Motion of Interacting People
                  from Head-Mounted Devices
                </papertitle>
              </a>
              <br>
              <a href="https://vlg.inf.ethz.ch/team/Siwei-Zhang.html">Siwei Zhang</a>,
              <strong>Qianli Ma</strong>,
              <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
              <a href="https://ethz.ch/en.html">Zhiyin Qian</a>,
              <a href="https://taeinkwon.com/">Taein Kwon</a>,
              <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>,
              <a href="https://fbogo.github.io/">Federica Bogo</a>,
              <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu
                Tang</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://sanweiliti.github.io/egobody/egobody.html">Project Page</a>
              / <a href="https://egobody.ethz.ch/">Dataset</a>
              / <a href="https://github.com/sanweiliti/EgoBody">Code</a>
              / <a href="https://arxiv.org/abs/2112.07642">arXiv</a>
              / <a href="https://youtu.be/yA7BM7zWAKM">Video</a>
              / <button id="bib_button" class="link", onclick="showBib('EboBody_bib')">BibTex</button>
              <div id='EboBody_bib' hidden>
                <pre><code class="language-bibtex">@inproceedings{Egobody:ECCV:2022,
  title = {{EgoBody}: Human Body Shape and Motion of Interacting People from Head-Mounted Devices},
  author = {Zhang, Siwei and Ma, Qianli and Zhang, Yan and Qian, Zhiyin and Kwon, Taein and Pollefeys, Marc and Bogo, Federica and Tang, Siyu},
  booktitle = {European Conference on Computer Vision (ECCV)},
  month = oct,
  year = {2022}
}</code></pre>
              </div>

              <p></p>
              <p>A large-scale dataset of accurate 3D body shape, pose and motion of humans interacting in 3D scenes,
                with multi-modal streams from third-person and egocentric views, captured by Azure Kinects and a HoloLens2. </p>
                <!-- Captured with multiple Azure Kinets and a HoloLens2, EgoBody provides rich multi-modal streams in both third-person and egocentric views. </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/POP.png' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://qianlim.github.io/POP">
                <font color=#1772d0>  <papertitle>The Power of Points for Modeling Humans in Clothing</papertitle></font>
              </a>
              <br>
              <strong>Qianli Ma</strong>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
              <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
              <br>
              <em>ICCV</em>, 2021
              <br>
              <a href="https://qianlim.github.io/POP">Project Page</a> /
              <a href="https://github.com/qianlim/POP">Code</a> /
              <a href="https://pop.is.tue.mpg.de/">Dataset</a> /
              <a href="https://arxiv.org/abs/2109.01137">arXiv</a> /
              <a href="https://youtu.be/JY5OI74yJ4w">Video</a> / 
              <button id="bib_button" class="link", onclick="showBib('POP_bib')">BibTex</button>
              <div id='POP_bib' hidden>           
                <pre><code>@inproceedings{POP:ICCV:2021,
  title = {The Power of Points for Modeling Humans in Clothing},
  author = {Ma, Qianli and Yang, Jinlong and Tang, Siyu and Black, Michael J.},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages = {10974--10984},
  month = oct,
  year = {2021},
}</code></pre>
              </div>

              <p></p>
              <p>PoP — a point-based, unified model for multiple subjects and outfits that can turn a <b>single, static</b> 3D scan into an
                animatable avatar with natural pose-dependent clothing deformations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/MetaAvatar.png' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://neuralbodies.github.io/metavatar/">
                <papertitle>MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images</papertitle>
              </a>
              <br>
              
              <a href=https://taconite.github.io/>Shaofei Wang</a>, 
              <a href=https://markomih.github.io/>Marko Mihajlovic</a>, 
              <strong>Qianli Ma</strong>, 
              <a href=http://www.cvlibs.net/>Andreas Geiger</a>, 
              <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
              <br>
              <em>NeurIPS</em>, 2021
              <br>
              <a href="https://neuralbodies.github.io/metavatar/">Project Page</a> /
              <a href="https://github.com/taconite/MetaAvatar-release">Code</a> /
              <a href="https://arxiv.org/abs/2106.11944">arXiv</a> /
              <a href="https://youtu.be/AwOwdKxuBXE">Video</a> / 
              
              <button id="bib_button" class="link", onclick="showBib('MetaAvatar_bib')">BibTex</button>
              <div id='MetaAvatar_bib' hidden>
              <pre><code>@inproceedings{MetaAvatar:NeurIPS:2021,
  title = {{MetaAvatar}: Learning Animatable Clothed Human Models from Few Depth Images},
  author={Wang, Shaofei and Mihajlovic, Marko and Ma, Qianli and Geiger, Andreas and Tang, Siyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2810--2822},
  year={2021},
}</code></pre>
              </div>
              <p></p>
              <!-- <p>A multi-subject, articulated, neural signed distance field model for clothed humans, which can fast -->
                <!-- create an avatar of unseen subjects from as few as 8 monocular depth images.</p> -->
              <p>Creating an avatar of unseen subjects from as few as eight monocular <b>depth images</b> using a meta-learned, multi-subject, articulated,
                neural signed distance field model for clothed humans.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/SCALE.png' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://qianlim.github.io/SCALE">
                <papertitle>SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements</papertitle>
              </a>
              <br>
              <strong>Qianli Ma</strong>, 
              <a href=https://shunsukesaito.github.io/>Shunsuke Saito</a>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
              <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://qianlim.github.io/SCALE">Project Page</a> /
              <a href="https://github.com/qianlim/SCALE">Code</a> /
              <a href="https://arxiv.org/abs/2104.07660">arXiv</a> /
              <a href="https://youtu.be/-EvWqFCUb7U">Video</a> /
              <button id="bib_button" class="link", onclick="showBib('SCALE_bib')">BibTex</button>
              <div id='SCALE_bib' hidden>
                <pre><code>@inproceedings{SCALE:CVPR:2021,
  title = {{SCALE}: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements},
  author = {Ma, Qianli and Saito, Shunsuke and Yang, Jinlong and Tang, Siyu and Black, Michael J.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {16082-16093},
  year = {2021},
  month = jun,
}</code></pre>
              </div>
              
              <p></p>
              <p>Modeling pose-dependent shapes of clothed humans explicitly with hundreds of articulated surface elements:
                 the clothing deforms naturally even in the presence of topological change.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/scanimate.png' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://scanimate.is.tue.mpg.de/">
                <papertitle>SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks</papertitle>
              </a>
              <br>
              <a href=https://shunsukesaito.github.io/>Shunsuke Saito</a>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
              <strong>Qianli Ma</strong>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
              <br>
              <em>CVPR</em>, 2021 &nbsp <font color="red"><strong>(Best Paper nominee)</strong></font>
              <br>
              <a href="https://scanimate.is.tue.mpg.de/">Project Page</a> /
              <a href="https://github.com/shunsukesaito/SCANimate">Code</a> /
              <a href="https://arxiv.org/abs/2104.03313">arXiv</a> / 
              <a href="https://youtu.be/EeNFvmNuuog">Video</a> /
              <button id="bib_button" class="link", onclick="showBib('SCANimate_bib')">BibTex</button>
              <div id='SCANimate_bib' hidden>
                <pre><code>@inproceedings{SCANimate:CVPR:2021,
  title={{SCANimate}: Weakly Supervised Learning of Skinned Clothed Avatar Networks},
  author={Saito, Shunsuke and Yang, Jinlong and Ma, Qianli and Black, Michael J},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={2886--2897},
  year={2021},
  month=jun,
}</code></pre>
              </div>
              <p></p>
              <!-- <p>Creating an animatable avatar with pose-dependent clothing deformation from <b>raw scans</b> without template surface registration.</p> -->
              <p>Cycle-consistent implicit skinning fields + locally pose-aware implicit function = a 
                fully animatable avatar with implicit surface from raw scans without surface registration.</p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/PLACE.jpeg' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sanweiliti.github.io/PLACE/PLACE.html">
                <papertitle>PLACE: Proximity Learning of Articulation and Contact in 3D Environments</papertitle>
              </a>
              <br>

              <a href="https://vlg.inf.ethz.ch/team/Siwei-Zhang.html">Siwei Zhang</a>,
              <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
              <strong>Qianli Ma</strong>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>, 
              <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
              <br>
              <em>3DV</em>, 2020
              <br>
              <a href="https://sanweiliti.github.io/PLACE/PLACE.html">Project Page</a> /
              <a href="https://github.com/sanweiliti/PLACE">Code</a> / 
              <a href="https://arxiv.org/abs/2008.05570">arXiv</a> /
              <a href="https://youtu.be/zJ1hbtMHGrw">Video</a> / 
              <button id="bib_button" class="link", onclick="showBib('PLACE_bib')">BibTex</button>
              <div id='PLACE_bib' hidden>
              <pre><code>@inproceedings{PLACE:3DV:2020,
  title = {{PLACE}: Proximity Learning of Articulation and Contact in {3D} Environments},
  author = {Zhang, Siwei and Zhang, Yan and Ma, Qianli and Black, Michael J. and Tang, Siyu},
  booktitle = {International Conference on 3D Vision (3DV)},
  month = nov,
  year = {2020},
}</code></pre>
              </div>      
              <p></p>
              <p>An explicit representation for 3D person-​scene contact relations that enables 
                automated synthesis of realistic humans posed naturally in a given scene.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/CAPE.png' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://cape.is.tue.mpg.de/">
                <papertitle>Learning to Dress 3D People in Generative Clothing</papertitle>
              </a>
              <br>
              <strong>Qianli Ma</strong>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
              <a href=https://anuragranj.github.io/>Anurag Ranjan</a>, 
              <a href=http://morpheo.inrialpes.fr/people/pujades//>Sergi Pujades</a>, 
              <a href=http://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html>Gerard Pons-Moll</a>, 
              <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
              <br>
              <em>CVPR</em>, 2020
              <br>
              <a href="https://cape.is.tue.mpg.de/">Project Page</a> /
              <a href="https://github.com/QianliM/CAPE">Code</a> / 
              <a href="https://cape.is.tue.mpg.de/dataset.html">Dataset</a> /
              <a href="https://arxiv.org/abs/1907.13615">arXiv</a> / 
              <a href="https://youtu.be/e4W-hPFNwDE">Full Video</a> /
              <a href="https://youtu.be/NOEA-Rtq6vM">1-min Video</a> / 
              <button id="bib_button" class="link", onclick="showBib('CAPE_bib')">BibTex</button>
              <div id='CAPE_bib' hidden>
              <pre><code>inproceedings{CAPE:CVPR:20,
  title = {Learning to Dress {3D} People in Generative Clothing},
  author = {Ma, Qianli and Yang, Jinlong and Ranjan, Anurag and Pujades, Sergi and Pons-Moll, Gerard and Tang, Siyu and Black, Michael J.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={6468-6477},
  month = jun,
  year = {2020},
}</code></pre>
              </div>

              <p></p>
              <p><i>CAPE</i> &mdash; a graph-CNN-based generative model and a large-scale dataset
                 for 3D human meshes in clothing in varied poses and garment types.</p>
            </td>
          </tr>

              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
    
    <br><br>
    <tr style="padding:0px">
    <td>
      <p style="text-align:right;">
      <font size="2">
      Template adapted from <a href="https://jonbarron.info/"><font size="2">John Barron</font></a>.
      </font>
      </p>
    </td>
    <tr/>

  </table>
  <script type="text/javascript" src="show_bib.js"></script>
</body>
</html>
